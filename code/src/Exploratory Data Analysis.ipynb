{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# from data import combine_with_reverb, create_bertified_dataset\n",
    "from utils import read_data\n",
    "from copy import copy\n",
    "import numpy as np \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating intermediate data indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combine_with_reverb()\n",
    "create_bertified_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Question word family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_WORDS = ['what', 'which', 'where', 'when', 'why', 'who', 'how', 'whom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_word_id(string):\n",
    "    for item in string.strip().lower().split():\n",
    "        if item in QUESTION_WORDS:\n",
    "            return item\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('../data/train.xlsx'); valid_df = pd.read_excel('../data/valid.xlsx'); test_df = pd.read_excel('../data/test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting number of rels and args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = pd.concat([train_df, valid_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_ent_rel(dataframe):\n",
    "    arg1 = [eval(item)[0] for item in dataframe['triple'].to_list()]\n",
    "    arg2 = [eval(item)[2] for item in dataframe['triple'].to_list()]\n",
    "    rel = [eval(item)[1] for item in dataframe['triple'].to_list()]\n",
    "    print(f'Unique arg1 : {len(set(arg1))}')\n",
    "    print(f'Unique arg2 : {len(set(arg2))}')\n",
    "    print(f'Unique rel : {len(set(rel))}')\n",
    "    print(f'Unique args : {len(set(arg1+arg2))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_ent_rel(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_ent_rel(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_ent_rel(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_ent_rel(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of question word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_words_distribution(dataframe):\n",
    "    dataframe['question words'] = dataframe['Question'].astype(str).apply(question_word_id)\n",
    "    print(dataframe.groupby(['question words'])['Meaningful'].count())\n",
    "    ax = dataframe.groupby(['question words'])['Meaningful'].count().plot.bar(x='lab', y='val', rot=0)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_question_words_distribution(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_words_distribution(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_words_distribution(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = pd.read_excel('../results/null.xlsx')\n",
    "get_question_words_distribution(null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Unique words in Qeustions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_of_unique_words(dataframe):\n",
    "    tokenizer = lambda string:string.strip().lower().split()\n",
    "    tokenized_questions = dataframe['Question'].astype(str).apply(tokenizer).to_list()\n",
    "    flatten_tokenized_questions = [item for sublist in tokenized_questions for item in sublist]\n",
    "    return len(set(flatten_tokenized_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_count_of_unique_words(train_df))\n",
    "print(get_count_of_unique_words(valid_df))\n",
    "print(get_count_of_unique_words(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question length histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(dataframe):\n",
    "    tokenizer = lambda string:len(string.strip().lower().split())\n",
    "    dataframe['length'] = dataframe['Question'].astype(str).apply(tokenizer)\n",
    "    ax = dataframe.hist(column=['length'], bins=50, grid=True)\n",
    "    ax[0, 0].set_xlim(1, 16)\n",
    "    return dataframe['length']\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = get_length(train_df)\n",
    "va = get_length(valid_df)\n",
    "te = get_length(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## length of Train, Valid, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = read_data()\n",
    "print(len(train[0]), len(valid[0]), len(test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining relation component count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation_component_count(dataset):\n",
    "    results = []\n",
    "    for item in dataset[1]:\n",
    "#         print(item[2:])\n",
    "        temp = ''.join(map(str, item[2:].tolist())).split('0')\n",
    "        temp = list(filter(lambda item:item!='', temp))\n",
    "        results.append(len(temp))\n",
    "    components = []\n",
    "    occurrences = []\n",
    "    for item in set(results):\n",
    "        components.append(str(item))\n",
    "        occurrences.append(results.count(item))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.bar(components,occurrences)\n",
    "    plt.show() \n",
    "    return components,occurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = get_relation_component_count(train)\n",
    "va = get_relation_component_count(valid)\n",
    "te = get_relation_component_count(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining length of relation based on whole question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation_length(dataset):\n",
    "    results = []\n",
    "    for tok, rel in zip(dataset[0], dataset[1]):\n",
    "        length = sum((tok!=0).astype(int))\n",
    "        temp = sum(rel[2:])/length\n",
    "        results.append(temp)\n",
    "    results = map(lambda item:round(item, 1), results)\n",
    "    results = list(results)\n",
    "    components = []\n",
    "    occurrences = []\n",
    "    for item in sorted(set(results)):\n",
    "        components.append(str(item))\n",
    "        occurrences.append(results.count(item))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "#     ax.set_xticks(ax.get_xticks()[::2])\n",
    "    ax.bar(components,occurrences)\n",
    "    plt.show() \n",
    "    return components,occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = get_relation_length(train); va = get_relation_length(valid); te = get_relation_length(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refrence = pd.read_excel('../data/intermediate.xlsx')\n",
    "refrence.Question = refrence.Question.apply(lambda x:str(x).lower().strip())\n",
    "error_dataframe = {\n",
    "                    'Question':[],\n",
    "                    'candidates':[],\n",
    "                    'actual':[],\n",
    "                    'node':[],\n",
    "                    'edge':[]\n",
    "                }\n",
    "with open('../results/Valid_Set_Without.txt', 'r') as res:\n",
    "    for line in res:\n",
    "        if line.find('Question')!=-1:\n",
    "            temp = eval(line.split(': ')[1].strip())\n",
    "            error_dataframe['Question'].append(' '.join(temp))\n",
    "        elif line.find('Sorted candidates')!=-1:\n",
    "            error_dataframe['candidates'].append(eval(line.split(': ')[1].strip()))\n",
    "        elif line.find('Node: ')!=-1:\n",
    "            line = line.replace(', Edge','').split(': ')\n",
    "            error_dataframe['node'].append(line[1])\n",
    "            error_dataframe['edge'].append(line[2].strip())\n",
    "        elif line.find('Actual line number')!=-1:\n",
    "            error_dataframe['actual'].append(eval(line.split(': ')[1].strip()))\n",
    "error_dataframe['Meaningful'] = [1 for _ in error_dataframe['actual']]\n",
    "# for k,v in error_dataframe.items():\n",
    "#     print(k, len(v), v[:5])\n",
    "#     error_dataframe[k]=v[:5350]\n",
    "error_df = pd.DataFrame(error_dataframe)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis = pd.merge(error_df, refrence, how='inner', on='Question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "empty_condidates = error_analysis[error_analysis.candidates.apply(lambda x:len(x)==0)][['Question', 'node', 'edge', 'triple', 'Reverb_no']]\n",
    "empty_condidates['triple'] = empty_condidates.triple.apply(lambda x:list(str(item).lower() for item in eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_condidates.to_excel('empty_candidates.xlsx')\n",
    "empty_condidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "node_precision, edge_precision = [], []\n",
    "for index, row in empty_condidates.iterrows():\n",
    "  try:\n",
    "    \n",
    "    temp = max([fuzz.ratio(item, row['node']) for item in row['triple']])\n",
    "    node_precision.append(temp) \n",
    "    temp = max([fuzz.ratio(item, row['edge']) for item in row['triple']])\n",
    "    edge_precision.append(temp) \n",
    "    \n",
    "  except Exception as e:\n",
    "    # raise e\n",
    "    pass\n",
    "  # break \n",
    "print(sum(node_precision)/len(node_precision))\n",
    "print(sum(edge_precision)/len(edge_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_rank(key,dic):\n",
    "    try:\n",
    "        return dic[key]/float(sum(dic.values()))\n",
    "    except:\n",
    "        return 1/1000\n",
    "def get_mean_rec_rank(dataframe):\n",
    "    dataframe['rr'] = dataframe.apply(lambda row:rec_rank(row['actual'],Counter([item[0] for item in row['candidates']])), axis=1)\n",
    "    print(dataframe['rr'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_rec_rank(error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_was_born(string):\n",
    "    tokenized = string.strip().lower().split()\n",
    "    if ('was' in tokenized) and ('born' in tokenized):\n",
    "            return True\n",
    "    return False\n",
    "error_df['was born']=error_df['Question'].apply(get_was_born)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_null_candidates = lambda candidates:len(candidates)==0\n",
    "null_questions = copy(error_df[error_df['candidates'].apply(get_null_candidates)])\n",
    "not_null_questions = copy(error_df[~error_df['candidates'].apply(get_null_candidates)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_questions.to_excel('../results/null.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question_words_distribution(null_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hit1(dataframe):\n",
    "    index_list = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        if row['candidates'][0][0]==row['actual']:\n",
    "                index_list.append(True)\n",
    "        else:\n",
    "            index_list.append(False)\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_questions[get_all_hit1(not_null_questions)].to_excel('../results/hit1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_questions[[not elem for elem in get_all_hit1(not_null_questions)]].to_excel('../results/not_null_not_hit1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Unique rels and args in Reverb tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\git\\reverb_wikipedia_tuples-1.1.txt', sep='\\t', header=None)\n",
    "reverb_columns_name = ['ExID', 'arg1', 'rel', 'arg2', 'narg1', 'nrel', 'narg2', 'csents', 'conf', 'urls']\n",
    "df.columns = reverb_columns_name\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Unique arg1 : {len(df[\"arg1\"].unique())}')\n",
    "print(f'Unique arg2 : {len(df[\"arg2\"].unique())}')\n",
    "print(f'Unique rel : {len(df[\"rel\"].unique())}')\n",
    "print(f'Unique args : {len(set(df[\"arg1\"].unique().tolist()+df[\"arg2\"].unique().tolist()))}')\n",
    "print(f'Unique narg1 : {len(df[\"narg1\"].unique())}')\n",
    "print(f'Unique narg2 : {len(df[\"narg2\"].unique())}')\n",
    "print(f'Unique nrel : {len(df[\"nrel\"].unique())}')\n",
    "print(f'Unique nargs : {len(set(df[\"narg1\"].unique().tolist()+df[\"narg2\"].unique().tolist()))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_vocab = df[\"narg1\"].unique().tolist()+df[\"narg2\"].unique().tolist()+df[\"nrel\"].unique().tolist()\n",
    "normal_vocab = list(map(lambda x:x.split(), normal_vocab))\n",
    "normal_vocab = [item for sublist in normal_vocab for item in sublist]\n",
    "print(len(set(normal_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = df[\"arg1\"].unique().tolist()+df[\"arg2\"].unique().tolist()+df[\"rel\"].unique().tolist()\n",
    "vocab = list(map(lambda x:x.split(), vocab))\n",
    "vocab = [item for sublist in vocab for item in sublist]\n",
    "print(len(set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8230408554867079 0.8347173823574736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_excel(r'.xlsx', engine ='openpyxl')\n",
    "valid = pd.read_excel(r'.xlsx', engine ='openpyxl')\n",
    "rr = lambda row: 1/10000 if int(row['Reverb_no']) not in [item[0] for item in eval(row['sys'])] else 1/(1+[item[0] for item in eval(row['sys'])].index(row['Reverb_no']))\n",
    "test['RR'] = test.apply(rr, axis=1)\n",
    "valid['RR'] = valid.apply(rr, axis=1)\n",
    "print(test['RR'].mean(), valid['RR'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
