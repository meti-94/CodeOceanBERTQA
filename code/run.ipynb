{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16r8uYp2_m4h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16r8uYp2_m4h",
    "outputId": "63af24c7-fac9-4831-d3c3-14be538f2c71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers -q\n",
    "!pip install pandas -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install openpyxl -q\n",
    "!pip install tabulate -q\n",
    "!pip install PatternLite -q\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf91d035",
   "metadata": {
    "id": "bf91d035"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19f00800",
   "metadata": {
    "id": "19f00800"
   },
   "outputs": [],
   "source": [
    "# check if Bertified data exists\n",
    "if not os.path.isfile('../data/Bertified/entities.npy'):\n",
    "    reverb_lines = read_reverb('../data/Reverb1.1/reverb_wikipedia_tuples-1.1.txt')\n",
    "    questions = pd.read_excel('../data/ReverbSQA/Final_Sheet_990824.xlsx', sheet_name=1, engine='openpyxl')\n",
    "    index = get_tuple_frequency(reverb_lines, questions)\n",
    "    index[index['Frequency']<10].to_excel('../data/ProcessedQuestions/normalized_questions.xlsx')\n",
    "    combine_with_reverb(questions_path='../data/ProcessedQuestions/normalized_questions.xlsx', \n",
    "                    reverb_path='../data/Reverb1.1/reverb_wikipedia_tuples-1.1.txt')\n",
    "    create_bertified_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "369878b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/capsule/results/output.txt', 'w') as f:\n",
    "    f.write('Start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4740e484",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "4740e484",
    "outputId": "7bd513af-8a40-479c-f5b9-9cd6bcbebe6d"
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# table 1 output\n",
    "\n",
    "print('\\n\\n', '#'*20, \"Table 1\".upper(), '#'*20)\n",
    "print(pd.read_excel('../data/Intermediate/train.xlsx').sample(6)[['triple', 'Question']])\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41b66aec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41b66aec",
    "outputId": "63b2731a-aff9-404f-fbe5-9c5e1d21e435"
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# table 2 output\n",
    "\n",
    "print('\\n\\n', '#'*20, \"Table 2\".upper(), '#'*20)\n",
    "df = pd.read_csv(r'../data/Reverb1.1/reverb_wikipedia_tuples-1.1.txt', sep='\\t', header=None)\n",
    "reverb_columns_name = ['ExID', 'arg1', 'rel', 'arg2', 'narg1', 'nrel', 'narg2', 'csents', 'conf', 'urls']\n",
    "df.columns = reverb_columns_name\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f'#Triples\\t\\t:\\t{len(df)}')\n",
    "print(f'#Relations\\t\\t:\\t{len(df[\"rel\"].unique())}')\n",
    "print(f'#Entity 1\\t\\t:\\t{len(df[\"arg1\"].unique())}')\n",
    "print(f'#Entity 2\\t\\t:\\t{len(df[\"arg2\"].unique())}')\n",
    "print(f'Total Unique Entities\\t:\\t{len(set(df[\"arg1\"].unique().tolist()+df[\"arg2\"].unique().tolist()))}')\n",
    "vocab = df[\"arg1\"].unique().tolist()+df[\"arg2\"].unique().tolist()+df[\"rel\"].unique().tolist()\n",
    "vocab = list(map(lambda x:x.split(), vocab))\n",
    "vocab = [item for sublist in vocab for item in sublist]\n",
    "print(f'Vocabulary Size\\t\\t:\\t{len(set(vocab))}')\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f11e164e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f11e164e",
    "outputId": "0d03685c-01e5-4108-9ded-219ba3ddf8dc"
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# table 3 output\n",
    "\n",
    "print('\\n\\n', '#'*20, \"Table 3\".upper(), '#'*20)\n",
    "train_df = pd.read_excel('../data/Intermediate/train.xlsx'); valid_df = pd.read_excel('../data/Intermediate/valid.xlsx'); test_df = pd.read_excel('../data/Intermediate/test.xlsx')\n",
    "\n",
    "\n",
    "def get_unique_ent_rel(dataframe):\n",
    "    arg1 = [eval(item)[0] for item in dataframe['triple'].to_list()]\n",
    "    arg2 = [eval(item)[2] for item in dataframe['triple'].to_list()]\n",
    "    rel = [eval(item)[1] for item in dataframe['triple'].to_list()]\n",
    "    print(f'Number of Questions\\t:\\t{len(dataframe)}')\n",
    "    print(f'Entity 1\\t:\\t{len(set(arg1))}')\n",
    "    print(f'Entity 2\\t:\\t{len(set(arg2))}')\n",
    "    print(f'Relations\\t:\\t{len(set(rel))}')\n",
    "    print(f'Total Unique Entities\\t:\\t{len(set(arg1+arg2))}')\n",
    "    tokenizer = lambda string:string.strip().lower().split()\n",
    "    tokenized_questions = dataframe['Question'].astype(str).apply(tokenizer).to_list()\n",
    "    flatten_tokenized_questions = [item for sublist in tokenized_questions for item in sublist]\n",
    "    print(f'Unique Words\\t:\\t{len(set(flatten_tokenized_questions))}')\n",
    "\n",
    "print(\"*** Training ***\")\n",
    "get_unique_ent_rel(train_df)\n",
    "print(\"\\n*** Validation ***\")\n",
    "get_unique_ent_rel(valid_df)\n",
    "print(\"\\n*** Test ***\")\n",
    "get_unique_ent_rel(test_df)\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25a1f3e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25a1f3e9",
    "outputId": "9c8ee33f-fbb5-4100-c3ba-88034e8b76d9"
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "print('\\n\\n', '#'*20, \"Table 5 GRU\".upper(), '#'*20)\n",
    "# table 5 output {GRU's row}\n",
    "!python3 ./BuboQA/entities/train.py  --entity_detection_mode GRU \\\n",
    "                                    --fix_embed --data_dir ../data/SimpleQuestionNotationEntity \\\n",
    "                                    --batch_size 256 \\\n",
    "                                    --vector_cache ../data/Cache/sq_glove300d.pt \\\n",
    "\n",
    "print('\\n\\n', '#'*20, \"Table 5 LSTM\".upper(), '#'*20)\n",
    "# table 5 output {GRU's row}\n",
    "!python3 ./BuboQA/entities/train.py  --entity_detection_mode LSTM \\\n",
    "                                    --fix_embed --data_dir ../data/SimpleQuestionNotationEntity \\\n",
    "                                    --batch_size 256 \\\n",
    "                                    --vector_cache ../data/Cache/sq_glove300d.pt \\\n",
    "\n",
    "print('\\n\\n', '#'*20, \"Table 6 LSTM\".upper(), '#'*20)\n",
    "# table 6 output {LSTM's row}\n",
    "!python3 ./BuboQA/relations/train.py  --relation_prediction_mode LSTM \\\n",
    "                                     --fix_embed --data_dir ../data/SimpleQuestionNotationRelation \\\n",
    "                                     --batch_size 256 \\\n",
    "                                     --vector_cache ../data/Cache/sq_glove300d.pt\n",
    "\n",
    "print('\\n\\n', '#'*20, \"Table 6 CNN\".upper(), '#'*20)\n",
    "# table 6 output {CNN's row}\n",
    "!python3 ./BuboQA/relations/train.py  --relation_prediction_mode CNN \\\n",
    "                                     --fix_embed --data_dir ../data/SimpleQuestionNotationRelation \\\n",
    "                                     --batch_size 256 \\\n",
    "                                     --vector_cache ../data/Cache/sq_glove300d.pt\n",
    "\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8e444ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8e444ba",
    "outputId": "865ee318-45e2-4f69-908a-907f47ba2c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/capsule/code/src\n",
      "\n",
      "\n",
      " #################### TABLE 7 & 8 TEST ####################\n",
      "INFO:root:\n",
      "\n",
      "############# Fold Number 1 #############\n",
      "\n",
      "\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140076598681024 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d.lock\n",
      "DEBUG:filelock:Lock 140076598681024 acquired on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 570\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 80.9kB/s]\n",
      "DEBUG:filelock:Attempting to release lock 140076598681024 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d.lock\n",
      "DEBUG:filelock:Lock 140076598681024 released on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140076598835904 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a.lock\n",
      "DEBUG:filelock:Lock 140076598835904 acquired on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn-lfs.huggingface.co:443 \"GET /bert-base-uncased/097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1679593300&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL2JlcnQtYmFzZS11bmNhc2VkLzA5NzQxNzM4MWQ2YzcyMzBiZDllMzU1NzQ1NmQ3MjZkZTZlODMyNDVlYzhiMjRmNTI5ZjYwMTk4YTY3YjIwM2E~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjc5NTkzMzAwfX19XX0_&Signature=ehN1dx89xwbZv4pg4yq2T3kKjGN8SgiM7Svezt7fs1yCxUMgdXcXIRk6jtOubJh6cg7UskKfL7-~Lxo7fSYWYYfiAI0IW5YwncYgbwTLqG1suTIAT6kCkaiqZeclWjwga51ExlepEygn-hQCGf-yAemPsWdRB7QBALJ1MvRNBPDTUjL8QJiPNJms0b6ilMdlfFWDlb3McN7oHbnQKqLh82sK6Yo54ycCpESkMyeg6Zc9fc-moOd2vxVt4-WVDN0g4ZSyzHQ~fEw20HD8Gfu05XjBn91ivIKb-zHxSpJoPAvLeZbhqMrms9sPv6TO9jQ3drsSEvvTw7p7lByuXqclUA__&Key-Pair-Id=KVTP0A1DKRTAX HTTP/1.1\" 200 440473133\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440M/440M [00:01<00:00, 355MB/s]\n",
      "DEBUG:filelock:Attempting to release lock 140076598835904 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a.lock\n",
      "DEBUG:filelock:Lock 140076598835904 released on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140076589702640 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n",
      "DEBUG:filelock:Lock 140076589702640 acquired on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 231508\n",
      "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 46.2MB/s]\n",
      "DEBUG:filelock:Attempting to release lock 140076589702640 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n",
      "DEBUG:filelock:Lock 140076589702640 released on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 140076598747392 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/a661b1a138dac6dc5590367402d100765010ffd6.lock\n",
      "DEBUG:filelock:Lock 140076598747392 acquired on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/a661b1a138dac6dc5590367402d100765010ffd6.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 28\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 13.0kB/s]\n",
      "DEBUG:filelock:Attempting to release lock 140076598747392 on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/a661b1a138dac6dc5590367402d100765010ffd6.lock\n",
      "DEBUG:filelock:Lock 140076598747392 released on /root/.cache/huggingface/hub/models--bert-base-uncased/blobs/a661b1a138dac6dc5590367402d100765010ffd6.lock\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:root:Train Dataset Contains 10630 Samples.\n",
      "INFO:root:Valid Dataset Contains 1876 Samples.\n",
      "INFO:root:Test Dataset Contains 4169 Samples.\n",
      "Train Epoch Number 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:26<00:00,  2.03it/s]\n",
      "INFO:root:Epoch number: 1 Train Loss is equal: 445.98876953125\n",
      "Eval Epoch Number 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.93it/s]\n",
      "INFO:root:Epoch number: 1 Eval Loss is equal: 23.635337829589844\n",
      "Train Epoch Number 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:26<00:00,  2.02it/s]\n",
      "INFO:root:Epoch number: 2 Train Loss is equal: 32.26915740966797\n",
      "Eval Epoch Number 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.77it/s]\n",
      "INFO:root:Epoch number: 2 Eval Loss is equal: 18.677011489868164\n",
      "Train Epoch Number 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 46/54 [00:23<00:04,  1.97it/s]^C\n",
      "Train Epoch Number 3:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 46/54 [00:23<00:04,  1.93it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 402, in <module>\n",
      "    tl.train(train_dataloader, valid_dataloader, loss)\n",
      "  File \"train.py\", line 246, in train\n",
      "    self.optimizer.step()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 280, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/optimization.py\", line 455, in step\n",
      "    p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      " #################### TABLE 7 & 8 VALID ####################\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 363, in <module>\n",
      "    report_on = sys.argv[4]\n",
      "IndexError: list index out of range\n",
      "\n",
      "\n",
      " #################### TABLE 9 & 10 TEST ####################\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 363, in <module>\n",
      "    report_on = sys.argv[4]\n",
      "IndexError: list index out of range\n",
      "\n",
      "\n",
      " #################### TABLE 9 & 10 VALID ####################\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 363, in <module>\n",
      "    report_on = sys.argv[4]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "# table 7 & 8 & 9 & 10 output\n",
    "\n",
    "%cd /root/capsule/code/src\n",
    "print('\\n\\n', '#'*20, \"Table 7 & 8 Test\".upper(), '#'*20)\n",
    "!python3 train.py False NodeEdgeDetector rsq test\n",
    "print('\\n\\n', '#'*20, \"Table 7 & 8 Valid\".upper(), '#'*20)\n",
    "!python3 train.py False NodeEdgeDetector rsq valid\n",
    "print('\\n\\n', '#'*20, \"Table 9 & 10 Test\".upper(), '#'*20)\n",
    "!python3 train.py False NodeEdgeDetector sq test\n",
    "print('\\n\\n', '#'*20, \"Table 9 & 10 Valid\".upper(), '#'*20)\n",
    "!python3 train.py False NodeEdgeDetector sq valid\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d750b14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d750b14",
    "outputId": "23eadfcd-095e-4ec3-a1ea-397837667faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CodeOceanBERTQA/code/src\n",
      "2023-03-19 17:55:57.772869: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-19 17:55:58.686035: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-19 17:55:58.686160: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-19 17:55:58.686184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:root:Train Dataset Contains 9921 Samples.\n",
      "INFO:root:Valid Dataset Contains 1751 Samples.\n",
      "INFO:root:Test Dataset Contains 5003 Samples.\n",
      "Train Epoch Number 1: 100% 50/50 [00:24<00:00,  2.01it/s]\n",
      "INFO:root:Epoch number: 1 Train Loss is equal: 564.5966796875\n",
      "Eval Epoch Number 1: 100% 9/9 [00:01<00:00,  7.45it/s]\n",
      "INFO:root:Epoch number: 1 Eval Loss is equal: 42.186466217041016\n",
      "Predicting ...: 100% 51/51 [00:03<00:00, 13.72it/s]\n",
      "INFO:root:Dataset-wide F1, precision and recall:\n",
      "INFO:root:0.9832155477031802, 0.9858281665190434, 0.9806167400881057\n",
      "INFO:root:Averaged F1, precision and recall:\n",
      "INFO:root:0.9852048842436942, 0.9859493827513015, 0.9844615092416411\n",
      "INFO:root:Span accuracy\n",
      "INFO:root:0.9714171497101739\n",
      "INFO:root:Dataset-wide F1, precision and recall:\n",
      "INFO:root:0.9840074608821802, 0.9852666528325379, 0.98275148337243\n",
      "INFO:root:Averaged F1, precision and recall:\n",
      "INFO:root:0.9892942254452505, 0.988929658078169, 0.9896590617058348\n",
      "INFO:root:Span accuracy\n",
      "INFO:root:0.9396362182690385\n",
      "Question                    Node               Edge\n",
      "--------------------------  -----------------  --------------------\n",
      "Where was Bill Gates Born?  ['bill', 'gates']  ['was', 'born', '?']\n"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "# table 11 output\n",
    "\n",
    "%cd /root/capsule/code/src\n",
    "print('\\n\\n', '#'*20, \"Table 11 BERT-LSTM-CRF\".upper(), '#'*20)\n",
    "!python3 train.py False BertLSTMCRF rsq test\n",
    "print('\\n\\n', '#'*20, \"Table 11 BERT-CNN\".upper(), '#'*20)\n",
    "!python3 train.py False BertCNN rsq test\n",
    "print('\\n\\n', '#'*20, \"Table 11 Multi-Depth \".upper(), '#'*20)\n",
    "!python3 train.py False MultiDepthNodeEdgeDetector rsq test\n",
    "print('\\n\\n', '#'*20, \"Table 11 Fine_tune BERT\".upper(), '#'*20)\n",
    "!python3 train.py False NodeEdgeDetector rsq test\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "BaKtbZnXaVYV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaKtbZnXaVYV",
    "outputId": "c161dbdc-890f-499d-8f3d-9f2bf005d0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜../../data/Modelsâ€™: File exists\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "INFO:root:Train Dataset Contains 9921 Samples.\n",
      "INFO:root:Valid Dataset Contains 1751 Samples.\n",
      "INFO:root:Test Dataset Contains 5003 Samples.\n",
      "Train Epoch Number 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:03<00:00,  1.26s/it]\n",
      "INFO:root:Epoch number: 1 Train Loss is equal: 494.9828796386719\n",
      "Eval Epoch Number 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.99it/s]\n",
      "INFO:root:Epoch number: 1 Eval Loss is equal: 40.74281692504883\n",
      "Train Epoch Number 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:00<00:00,  1.20s/it]\n",
      "INFO:root:Epoch number: 2 Train Loss is equal: 35.747257232666016\n",
      "Eval Epoch Number 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:02<00:00,  3.02it/s]\n",
      "INFO:root:Epoch number: 2 Eval Loss is equal: 18.81749153137207\n",
      "Train Epoch Number 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:34<00:00,  1.44it/s]\n",
      "INFO:root:Epoch number: 3 Train Loss is equal: 21.92192268371582\n",
      "Eval Epoch Number 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:01<00:00,  6.07it/s]\n",
      "INFO:root:Epoch number: 3 Eval Loss is equal: 22.718732833862305\n",
      "Train Epoch Number 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:28<00:00,  1.74it/s]\n",
      "INFO:root:Epoch number: 4 Train Loss is equal: 16.37309455871582\n",
      "Eval Epoch Number 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:01<00:00,  6.02it/s]\n",
      "INFO:root:Epoch number: 4 Eval Loss is equal: 15.37909984588623\n",
      "Train Epoch Number 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:28<00:00,  1.74it/s]\n",
      "INFO:root:Epoch number: 5 Train Loss is equal: 13.871003150939941\n",
      "Eval Epoch Number 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:01<00:00,  5.95it/s]\n",
      "INFO:root:Epoch number: 5 Eval Loss is equal: 13.43352222442627\n",
      "Train Epoch Number 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:28<00:00,  1.76it/s]\n",
      "INFO:root:Epoch number: 6 Train Loss is equal: 11.202738761901855\n",
      "Eval Epoch Number 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:01<00:00,  6.00it/s]\n",
      "INFO:root:Epoch number: 6 Eval Loss is equal: 15.572461128234863\n",
      "Predicting ...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:04<00:00, 11.37it/s]\n",
      "INFO:root:Dataset-wide F1, precision and recall:\n",
      "INFO:root:0.9893156732891832, 0.9915044247787611, 0.9871365638766519\n",
      "INFO:root:Averaged F1, precision and recall:\n",
      "INFO:root:0.9875798743322974, 0.9892497834632554, 0.9859155935010423\n",
      "INFO:root:Span accuracy\n",
      "INFO:root:0.9822106735958425\n",
      "INFO:root:Dataset-wide F1, precision and recall:\n",
      "INFO:root:0.9927883785928713, 0.9930282322081867, 0.9925486408168898\n",
      "INFO:root:Averaged F1, precision and recall:\n",
      "INFO:root:0.99479667651929, 0.9942934239456328, 0.9953004387843492\n",
      "INFO:root:Span accuracy\n",
      "INFO:root:0.9734159504297422\n",
      "Question                    Node               Edge\n",
      "--------------------------  -----------------  --------------------\n",
      "Where was Bill Gates Born?  ['bill', 'gates']  ['was', 'born', '?']\n",
      "[Errno 2] No such file or directory: '/content/CodeOceanBERTQA/code/src'\n",
      "/root/capsule/code/src\n",
      "/bin/bash: python: command not found\n"
     ]
    }
   ],
   "source": [
    "%cd /root/capsule/code/src\n",
    "!mkdir ../../data/Models\n",
    "!python3 train.py True NodeEdgeDetector rsq test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcd83a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/capsule/code/src\n",
      "/usr/local/lib/python3.8/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "Traceback (most recent call last):\n",
      "  File \"evaluation.py\", line 2, in <module>\n",
      "    from graph import *\n",
      "  File \"/root/capsule/code/src/graph.py\", line 9, in <module>\n",
      "    from pattern.en import conjugate, lemma, lexeme,PRESENT,SG,PAST\n",
      "ModuleNotFoundError: No module named 'pattern'\n"
     ]
    }
   ],
   "source": [
    "%cd /root/capsule/code/src\n",
    "!python3 evaluation.py tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "VbVGCuHp9XK0",
   "metadata": {
    "id": "VbVGCuHp9XK0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.732560463721767, 0.8474915050969418, 0.8648810713571857, 0.878273036178293, 0.8924645212872276)\n",
      "(0.7572815533980582, 0.8669331810394061, 0.8800685322672759, 0.8897772701313535, 0.903483723586522)\n"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "# table 12 output\n",
    "\n",
    "%cd /root/capsule/code\n",
    "print('\\n\\n', '#'*20, \"Table 12 (Test, Valid)\".upper(), '#'*20)\n",
    "\n",
    "from src.utils import get_hit\n",
    "import pandas as pd\n",
    "test_df = pd.read_excel('../data/Intermediate/test.xlsx')\n",
    "actual = test_df['Reverb_no'].to_list()\n",
    "system_results = pd.read_excel('../data/Candidates/test_results.xlsx')['sys'].apply(lambda item:eval(item))\n",
    "print(get_hit(actual, system_results))\n",
    "test_df = pd.read_excel('../data/Intermediate/valid.xlsx')\n",
    "actual = test_df['Reverb_no'].to_list()\n",
    "system_results = pd.read_excel('../data/Candidates/valid_results.xlsx')['sys'].apply(lambda item:eval(item))\n",
    "print(get_hit(actual, system_results))\n",
    "\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8746ce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7810465810353977 0.801262243574989\n"
     ]
    }
   ],
   "source": [
    "%%capture cap --no-stderr\n",
    "# table 13 output\n",
    "\n",
    "%cd /root/capsule/code\n",
    "print('\\n\\n', '#'*20, \"Table 13 (Test, Valid)\".upper(), '#'*20)\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "test = pd.read_excel('../data/Candidates/test_results.xlsx', engine ='openpyxl')\n",
    "valid = pd.read_excel('../data/Candidates/valid_results.xlsx', engine ='openpyxl')\n",
    "rr = lambda row: 1/10000 if int(row['Reverb_no']) not in [item[0] for item in eval(row['sys'])] else 1/(1+[item[0] for item in eval(row['sys'])].index(row['Reverb_no']))\n",
    "test['RR'] = test.apply(rr, axis=1)\n",
    "valid['RR'] = valid.apply(rr, axis=1)\n",
    "print(test['RR'].mean(), valid['RR'].mean())\n",
    "with open('/root/capsule/results/output.txt', 'a') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b0956cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "import pandas as pd\n",
    "refrence = pd.read_excel('../data/Intermediate/test.xlsx')\n",
    "refrence.Question = refrence.Question.apply(lambda x:str(x).lower().strip())\n",
    "error_dataframe = {\n",
    "                    'Question':[],\n",
    "                    'candidates':[],\n",
    "                    'actual':[],\n",
    "                    'node':[],\n",
    "                    'edge':[]\n",
    "                }\n",
    "with open('../data/Detailed/Test_Set_With.txt', 'r') as res:\n",
    "    for line in res:\n",
    "        if line.find('Question')!=-1:\n",
    "            temp = eval(line.split(': ')[1].strip())\n",
    "            error_dataframe['Question'].append(' '.join(temp))\n",
    "        elif line.find('Sorted candidates')!=-1:\n",
    "            error_dataframe['candidates'].append(eval(line.split(': ')[1].strip()))\n",
    "        elif line.find('Node: ')!=-1:\n",
    "            line = line.replace(', Edge','').split(': ')\n",
    "            error_dataframe['node'].append(line[1])\n",
    "            error_dataframe['edge'].append(line[2].strip())\n",
    "        elif line.find('Actual line number')!=-1:\n",
    "            error_dataframe['actual'].append(eval(line.split(': ')[1].strip()))\n",
    "error_dataframe['Meaningful'] = [1 for _ in error_dataframe['actual']]\n",
    "# for k,v in error_dataframe.items():\n",
    "#     print(k, len(v), v[:5])\n",
    "#     error_dataframe[k]=v[:5350]\n",
    "error_df = pd.DataFrame(error_dataframe)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "443f922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis = pd.merge(error_df, refrence, how='inner', on='Question')\n",
    "error_analysis['candidates_index'] = error_analysis['candidates'].apply(lambda item:[i[0] for i in item])\n",
    "gh = lambda row:row['candidates_index'].index(row['actual']) if (row['actual'] in row['candidates_index']) else 100\n",
    "error_analysis['hit'] = error_analysis.apply(gh, axis=1)\n",
    "empty_condidates = error_analysis[(error_analysis['hit']<10)&(error_analysis['hit']>-1)][['Question', 'node', 'edge', 'normalized_triple', 'Reverb_no', 'hit']]\n",
    "empty_condidates['triple'] = empty_condidates.normalized_triple.apply(lambda x:list(str(item).lower() for item in eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9666d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3041\n",
      "98.5386386057218\n",
      "77.67181848076291\n"
     ]
    }
   ],
   "source": [
    "node_precision, edge_precision = [], []\n",
    "print(len(empty_condidates))\n",
    "for index, row in empty_condidates.iterrows():\n",
    "    temp = max([fuzz.ratio(item, row['node']) for item in row['triple']])\n",
    "    node_precision.append(temp) \n",
    "    temp = max([fuzz.ratio(item, row['edge']) for item in row['triple']])\n",
    "    edge_precision.append(temp) \n",
    "# print(min(node_precision))\n",
    "print(sum(node_precision)/len(node_precision))\n",
    "print(sum(edge_precision)/len(edge_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f923651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "assert faiss.get_num_gpus() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda3370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41f85d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.40541529454812\n",
      "92.34687156970362\n"
     ]
    }
   ],
   "source": [
    "98.4924990852543\n",
    "78.15331137943652\n",
    "\n",
    "98.52592341579125\n",
    "77.76821416468994\n",
    "\n",
    "\n",
    "98.54240106418357\n",
    "77.73761223811107\n",
    "\n",
    "98.5386386057218\n",
    "77.66425517921736\n",
    "\n",
    "98.54151624548736\n",
    "77.65408598621595\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
